{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Requirement already satisfied: pip in /home/ubuntu/.local/lib/python3.7/site-packages (22.2.2)\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Requirement already satisfied: pyspark in /home/ubuntu/.local/lib/python3.7/site-packages (3.3.0)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /home/ubuntu/.local/lib/python3.7/site-packages (from pyspark) (0.10.9.5)\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Requirement already satisfied: pandas==1.3 in /home/ubuntu/.local/lib/python3.7/site-packages (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/anaconda/lib/python3.7/site-packages (from pandas==1.3) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/anaconda/lib/python3.7/site-packages (from pandas==1.3) (2018.9)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/ubuntu/.local/lib/python3.7/site-packages (from pandas==1.3) (1.21.6)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/anaconda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas==1.3) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "# Package import\n",
    "!pip install --user --upgrade pip\n",
    "!pip install --user pyspark\n",
    "!pip install --user pandas==1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Basic Functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "\n",
    "# PySpark\n",
    "import pyspark\n",
    "from pyspark.sql.functions import isnan, isnull\n",
    "from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
    "from pyspark.sql import Window, SparkSession, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Imputer,VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fake dataset of random names and their age\n",
    "\n",
    "age = list(range(10,60))\n",
    "random.shuffle(age)\n",
    "name =[]\n",
    "\n",
    "for i in range(50):\n",
    "    name.append(''.join(random.sample(string.ascii_lowercase, 8)))\n",
    "    \n",
    "df = pd.DataFrame({'Name':name,'Age':age})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://run-6310eda7e5cd781cc2223030-g4t28:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practise</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f46e8fb3a50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark =SparkSession.builder.appName('Practise').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      "\n",
      "+--------+---+\n",
      "|    Name|Age|\n",
      "+--------+---+\n",
      "|lbgifhdt| 57|\n",
      "|ygtjswmq| 18|\n",
      "|lqmxnzuy| 37|\n",
      "|hyafrdte| 29|\n",
      "|wiypsqxf| 50|\n",
      "|zhskvjfm| 40|\n",
      "|oedifgrj| 20|\n",
      "|znrueqlw| 27|\n",
      "|iemtualb| 59|\n",
      "|sylxtova| 32|\n",
      "|kncvidua| 55|\n",
      "|iltknbcg| 21|\n",
      "|hictpdlx| 15|\n",
      "|juzwedmy| 11|\n",
      "|frdvxqaj| 26|\n",
      "|oymsacjw| 56|\n",
      "|cjqelkvf| 17|\n",
      "|gycvmsnw| 51|\n",
      "|ulabpnfr| 48|\n",
      "|oskjifzl| 22|\n",
      "+--------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Spark dataframe and display contents\n",
    "df_pyspark = spark.createDataFrame(df) \n",
    "df_pyspark.printSchema()\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    Name|Age|\n",
      "+--------+---+\n",
      "|lbgifhdt| 57|\n",
      "|ygtjswmq| 18|\n",
      "|lqmxnzuy| 37|\n",
      "|hyafrdte| 29|\n",
      "|wiypsqxf| 50|\n",
      "+--------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select('Name','Age').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'), ('Age', 'bigint')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------------------+\n",
      "|summary|    Name|               Age|\n",
      "+-------+--------+------------------+\n",
      "|  count|      50|                50|\n",
      "|   mean|    null|              34.5|\n",
      "| stddev|    null|14.577379737113251|\n",
      "|    min|adyfhsjk|                10|\n",
      "|    max|zqwhxftd|                59|\n",
      "+-------+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Columns by manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-----------------+\n",
      "|    Name|Age|Age after 2 years|\n",
      "+--------+---+-----------------+\n",
      "|lbgifhdt| 57|               59|\n",
      "|ygtjswmq| 18|               20|\n",
      "|lqmxnzuy| 37|               39|\n",
      "|hyafrdte| 29|               31|\n",
      "|wiypsqxf| 50|               52|\n",
      "+--------+---+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark=df_pyspark.withColumn('Age after 2 years',df_pyspark['Age']+2)\n",
    "df_pyspark.select('Name','Age','Age after 2 years').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       name  Experience  Salary       Position\n",
      "0  lbgifhdt           4   33117    Team Leader\n",
      "1  ygtjswmq           3   12682  Data Engineer\n",
      "2  lqmxnzuy           5   41422         Intern\n",
      "3  hyafrdte           9   51908    Team Leader\n",
      "4  wiypsqxf           3   15469            CFO\n"
     ]
    }
   ],
   "source": [
    "# Adding new column to pyspark dataframe\n",
    "jobs = ['Data Scientist','MLops','IT','Seed Physiologist','Intern','Team Leader','Data Engineer','Lab tech','CFO','CTO','CEO']\n",
    "exp_and_salary = []\n",
    "for i in range(0,len(age)):\n",
    "    exp_and_salary.append([name[i],random.randint(1,10),random.randint(10000,60000),random.choice(jobs)])\n",
    "\n",
    "exp_and_salary = pd.DataFrame(exp_and_salary,columns=['name','Experience','Salary','Position'])\n",
    "print(exp_and_salary.head())\n",
    "exp_and_salary = spark.createDataFrame(exp_and_salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing column names\n",
    "exp_and_salary=exp_and_salary.withColumnRenamed('name','Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging another pyspark dataframe\n",
    "new_df_pyspark= df_pyspark.join(exp_and_salary,[\"Name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      " |-- Experience: long (nullable = true)\n",
      " |-- Salary: long (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df_pyspark= new_df_pyspark.drop('Age after 2 years')\n",
    "new_df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---+\n",
      "|         Position|Age|\n",
      "+-----------------+---+\n",
      "|    Data Engineer| 18|\n",
      "|              CTO| 40|\n",
      "|              CFO| 32|\n",
      "|           Intern| 59|\n",
      "|Seed Physiologist| 55|\n",
      "|              CFO| 56|\n",
      "|              CFO| 50|\n",
      "|      Team Leader| 57|\n",
      "|              CTO| 20|\n",
      "|Seed Physiologist| 15|\n",
      "|              CEO| 11|\n",
      "|      Team Leader| 41|\n",
      "|              CTO| 33|\n",
      "|           Intern| 51|\n",
      "|              CEO| 42|\n",
      "|              CEO| 10|\n",
      "|         Lab tech| 22|\n",
      "|      Team Leader| 53|\n",
      "|               IT| 46|\n",
      "|               IT| 52|\n",
      "+-----------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df_pyspark.filter(\"salary<=35000\").select(['Position',\"Age\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+------+-----------------+\n",
      "|    Name|Age|Experience|Salary|         Position|\n",
      "+--------+---+----------+------+-----------------+\n",
      "|adyfhsjk| 33|         6| 11752|              CTO|\n",
      "|cgqjmbui| 30|         6| 33650|              CEO|\n",
      "|eyczapnl| 28|         9| 20846|      Team Leader|\n",
      "|fnadujlp| 42|         4| 14660|              CEO|\n",
      "|gbotirew| 23|         8| 33733|              CTO|\n",
      "|gslvrqbf| 16|         1| 17201|    Data Engineer|\n",
      "|gycvmsnw| 51|         5| 14288|           Intern|\n",
      "|hictpdlx| 15|         3| 20854|Seed Physiologist|\n",
      "|iemtualb| 59|         8| 32841|           Intern|\n",
      "|kncvidua| 55|         2| 14765|Seed Physiologist|\n",
      "+--------+---+----------+------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df_pyspark.filter((new_df_pyspark[\"salary\"]<=35000) & (new_df_pyspark['Age']>=15)).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+------+--------+\n",
      "|    Name|Age|Experience|Salary|Position|\n",
      "+--------+---+----------+------+--------+\n",
      "|cakixmbs| 14|         1| 47886|Lab tech|\n",
      "|juzwedmy| 11|         9| 27019|     CEO|\n",
      "|uhtzkepd| 13|         7| 39197|     CTO|\n",
      "+--------+---+----------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df_pyspark.filter(~(new_df_pyspark[\"salary\"]<=25000) & ~(new_df_pyspark['Age']>=15)).show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------------------+-----------------+------------------+-----------+\n",
      "|summary|    Name|               Age|       Experience|            Salary|   Position|\n",
      "+-------+--------+------------------+-----------------+------------------+-----------+\n",
      "|  count|      50|                50|               50|                50|         50|\n",
      "|   mean|    null|              34.5|              5.1|          33993.56|       null|\n",
      "| stddev|    null|14.577379737113251|2.844615393105696|15629.639763903317|       null|\n",
      "|    min|adyfhsjk|                10|                1|             11240|        CEO|\n",
      "|    max|zqwhxftd|                59|               10|             59815|Team Leader|\n",
      "+-------+--------+------------------+-----------------+------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Salary Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------+---------------+-----------+\n",
      "|         Position|max(Age)|max(Experience)|max(Salary)|\n",
      "+-----------------+--------+---------------+-----------+\n",
      "|         Lab tech|      54|             10|      47886|\n",
      "|      Team Leader|      57|             10|      51908|\n",
      "|              CTO|      40|              8|      53909|\n",
      "|Seed Physiologist|      55|              5|      54831|\n",
      "|           Intern|      59|              8|      59112|\n",
      "|            MLops|      38|              2|      49533|\n",
      "|              CFO|      56|             10|      56724|\n",
      "|              CEO|      47|              9|      59815|\n",
      "|   Data Scientist|      44|             10|      46810|\n",
      "|               IT|      58|              8|      48417|\n",
      "|    Data Engineer|      49|              9|      58496|\n",
      "+-----------------+--------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df_pyspark.groupBy('Position').max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------+---------------+-----------+\n",
      "|         Position|min(Age)|min(Experience)|min(Salary)|\n",
      "+-----------------+--------+---------------+-----------+\n",
      "|         Lab tech|      14|              1|      15660|\n",
      "|      Team Leader|      28|              2|      20846|\n",
      "|              CTO|      12|              1|      11752|\n",
      "|Seed Physiologist|      15|              2|      14765|\n",
      "|           Intern|      26|              2|      14288|\n",
      "|            MLops|      24|              2|      38682|\n",
      "|              CFO|      25|              3|      15469|\n",
      "|              CEO|      10|              1|      12583|\n",
      "|   Data Scientist|      27|              1|      42357|\n",
      "|               IT|      39|              1|      11240|\n",
      "|    Data Engineer|      16|              1|      12682|\n",
      "+-----------------+--------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df_pyspark.groupBy('Position').min().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------+---------------+------------------+\n",
      "|         Position|avg(Age)|avg(Experience)|       avg(Salary)|\n",
      "+-----------------+--------+---------------+------------------+\n",
      "|         Lab tech|    34.5|           4.75|          34668.75|\n",
      "|      Team Leader|    41.6|            6.8|           32339.2|\n",
      "|              CTO|   26.25|           5.25|         28727.875|\n",
      "|Seed Physiologist|   30.75|            3.5|          33410.75|\n",
      "|           Intern|    43.6|            4.8|           40890.0|\n",
      "|            MLops|    31.0|            2.0|           44107.5|\n",
      "|              CFO|    38.8|            6.4|           33766.6|\n",
      "|              CEO|    30.5|            5.0|33152.833333333336|\n",
      "|   Data Scientist|    35.5|            5.5|           44583.5|\n",
      "|               IT|   48.75|            5.0|           24307.5|\n",
      "|    Data Engineer|    24.6|            5.0|           37805.8|\n",
      "+-----------------+--------+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df_pyspark.groupBy('Position').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|         Position|count|\n",
      "+-----------------+-----+\n",
      "|         Lab tech|    4|\n",
      "|      Team Leader|    5|\n",
      "|              CTO|    8|\n",
      "|Seed Physiologist|    4|\n",
      "|           Intern|    5|\n",
      "|            MLops|    2|\n",
      "|              CFO|    5|\n",
      "|              CEO|    6|\n",
      "|   Data Scientist|    2|\n",
      "|               IT|    4|\n",
      "|    Data Engineer|    5|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df_pyspark.groupBy('Position').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vishualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to pandas and adding NaNs\n",
    "df_new = new_df_pyspark.toPandas()\n",
    "df_nulls = df_new.apply(lambda x: x.sample(frac=0.95))\n",
    "df_nulls.head(10)\n",
    "df_nulls.fillna('NaN Values',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Pyspark = spark.createDataFrame(df_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      " |-- Experience: long (nullable = true)\n",
      " |-- Salary: long (nullable = true)\n",
      "\n",
      "+--------+---+----------+------+\n",
      "|    Name|Age|Experience|Salary|\n",
      "+--------+---+----------+------+\n",
      "|osvfdtzb| 55|      null|  null|\n",
      "|osmqklvg| 50|      null|  null|\n",
      "|lpoyzwrg| 40|      null|  null|\n",
      "|nfljmepz| 47|      null|  null|\n",
      "|jxriuchk| 32|      null|  null|\n",
      "|exdyrmth| 27|      null|  null|\n",
      "|xauqdefn| 44|      null|  null|\n",
      "|yvgiaruj| 58|      null|  null|\n",
      "|prqwfcza| 22|      null|  null|\n",
      "|adsxwjlf| 38|      null|  null|\n",
      "|cbkrqytf| 34|      null|  null|\n",
      "|brptfnjk| 36|      null|  null|\n",
      "|onrmyksi| 15|      null|  null|\n",
      "|tzekfiwr| 14|      null|  null|\n",
      "|zhgfldny| 19|      null|  null|\n",
      "|tuxbmgqj| 59|      null|  null|\n",
      "|juerqpmc| 29|      null|  null|\n",
      "|jfwvtimz| 57|      null|  null|\n",
      "|lgvmbked| 24|      null|  null|\n",
      "|pvtyifar| 12|      null|  null|\n",
      "+--------+---+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Creating Na and replacing it\n",
    "sc=spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python MLib tutorial----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLib in Spark through DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer (\n",
    "inputCols= ['Age','Experience','salary'],\n",
    "outputCols = [\"{}_imputed\".format(c) for c in ['Age','Experience','salary']]).setStrategy('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureassembler = VectorAssembler(inputCols = ['Age','Experience'],outputCol = 'Independent_Feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = featureassembler.transform(df_pyspark)\n",
    "output.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pyspark = output.select(\"Independent_Feature\",\"salary\")\n",
    "final_pyspark.show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Split\n",
    "train_data,test_data=final_pyspark.randomSplit([0.75,0.25])\n",
    "regressor = LinearRegression(featuresCol = 'Independent_Feature',labelCol = \"salary\")\n",
    "regressor = regressor.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regressor.coefficients)\n",
    "print(regressor.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results=regressor.evaluate(test_data)\n",
    "pred_results.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results.meanAbsoluteError,pred_results.meanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pyspark and Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "cancer_df['target']=data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_df_pyspark = spark.createDataFrame(cancer_df) \n",
    "cancer_df_pyspark.printSchema()\n",
    "cancer_df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_df_pyspark.groupBy('target').count().show()\n",
    "# 1 means diagnosis of breast tissues benign (B)and 0 means, the diagnosis is malignant(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pyspark = output.select(\"Independent_Feature\",\"salary\")\n",
    "final_pyspark.show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "diabetes_df['target']=data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df_pyspark = spark.createDataFrame(diabetes_df) \n",
    "diabetes_df_pyspark.printSchema()\n",
    "diabetes_df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = list(diabetes_df_pyspark.columns)\n",
    "column_name.remove('target')\n",
    "featureassembler = VectorAssembler(inputCols = column_name,outputCol = 'Independent_Feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_diabetes_pyspark = featureassembler.transform(diabetes_df_pyspark).select(\"Independent_Feature\",\"target\")\n",
    "final_diabetes_pyspark.show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data=final_diabetes_pyspark.randomSplit([0.65,0.35])\n",
    "regressor = LinearRegression(featuresCol = 'Independent_Feature',labelCol = \"target\")\n",
    "regressor = regressor.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regressor.coefficients)\n",
    "print(regressor.intercept)\n",
    "pred_results=regressor.evaluate(test_data)\n",
    "pred_results.meanAbsoluteError,pred_results.meanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset was generated after the hackers hacked the servers to save the company data from such activities in future \n",
    "# forensic engineers. They grabbed some features that will give us relevant metadata about the type of hackers.\n",
    "\n",
    "# Here is a brief description of each feature:\n",
    "\n",
    "# Session connection time: This indicates the total time session existed in minutes.\n",
    "# Bytes transferred: will let us know how many megabytes were transferred during the session.\n",
    "# Kali trace used: This is a kind of flag variable that indicates whether the hacker used the Kali Linux operator.\n",
    "# Servers corrupted: How many servers got corrupted during the attack.\n",
    "# Pages corrupted: How many pages were accessed by them illegally.\n",
    "# Location: Though this meta information is also available with us, this one is of no use as hackers use VPNs\n",
    "# WPM typing speed: The typing speed of those criminals based on the available logs.\n",
    "# First, let’s understand what the company already knows. So, they are aware of the fact that 3 types of hackers might penetrate the attack. They are quite sure about the 2 of them, but they want us to know whether the third type of attacker was involved in this act of criminal or not.\n",
    "# One key thing we should know before moving forward, i.e., forensic engineers know that hackers trade-offs, which means the number of attacks was the same for each hacker. So if there are 3 types of hackers, three of them might have equally distributed the attacks. Otherwise, the third suspect would not have been involved this time.\n",
    "\n",
    "# Example: If all three types of attackers were the suspects, then for 100 attacks, each one will be responsible for 33."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/farhanafayez/PySpark-K-means-Clustering-ML.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark =SparkSession.builder.appName('Practise').getOrCreate()\n",
    "dataset = spark.read.csv(\"/mnt/PySpark/PySpark-K-means-Clustering-ML/hack_data.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vishualization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_group = dataset.groupBy(\"Location\").sum(\"Bytes Transferred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = loc_group.toPandas()[\"Location\"].values.tolist(),loc_group.toPandas()[\"sum(Bytes Transferred)\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40, 30), dpi=300)\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40, 30), dpi=300)\n",
    "plt.bar(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_group.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length((loc_group.select(\"Location\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = dataset.columns\n",
    "feature_cols.remove('Location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_assembler = VectorAssembler(inputCols =feature_cols,outputCol='features')\n",
    "final_data=vector_assembler.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol='features',outputCol='scaledFeatures',withStd=True,withMean=False)\n",
    "scalerModel =scaler.fit(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_final_data=scalerModel.transform(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_final_data..select(\"scaledFeatures\",\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions={}\n",
    "for k in range(2,9):\n",
    "    kmeans = KMeans(featuresCol='scaledFeatures',k=k)\n",
    "    model = kmeans.fit(cluster_final_data)\n",
    "    model = model.transform(cluster_final_data)\n",
    "    predictions[k]=model.groupBy('prediction').count()\n",
    "    k_evaluator = evaluator.evaluate(model)\n",
    "    print(\"With K={}\".format(k))\n",
    "    print(\"Error results  = \" + str(k_evaluator))\n",
    "    print('-'*53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in predictions.items():\n",
    "    print(\"With K={}\".format(key))\n",
    "    print((value.show()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inference: Here, we can see that an equal number of predictions exist in the case where several clusters are only 2; \n",
    "# hence we can conclude that the third type of attacker was not involved in hacking the company’s servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wssse_k3 = model_k3.computeCost(cluster_final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".select(\"Independent_Feature\",\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = ClusteringEvaluator()\n",
    "silhouette = evaluator.evaluate(model_k3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do one clustering\n",
    "# do one logistic reression\n",
    "# do one linear reression\n",
    "# do one classification\n",
    "DecisionTreeClassifier/Regressor (SPARK-19591), RandomForestClassifier/Regressor (SPARK-9478), GBTClassifier/Regressor (SPARK-9612), MulticlassClassificationEvaluator (SPARK-24101), RegressionEvaluator (SPARK-24102), BinaryClassificationEvaluator (SPARK-24103), BisectingKMeans (SPARK-30351), KMeans (SPARK-29967) and GaussianMixture (SPARK-30102).\n",
    "https://spark.apache.org/docs/latest/ml-guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
