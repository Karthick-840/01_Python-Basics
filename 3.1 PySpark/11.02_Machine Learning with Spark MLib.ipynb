{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Requirement already satisfied: pip in /home/ubuntu/.local/lib/python3.7/site-packages (22.2.2)\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Requirement already satisfied: pyspark in /home/ubuntu/.local/lib/python3.7/site-packages (3.3.0)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /home/ubuntu/.local/lib/python3.7/site-packages (from pyspark) (0.10.9.5)\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Requirement already satisfied: pandas==1.3 in /home/ubuntu/.local/lib/python3.7/site-packages (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/anaconda/lib/python3.7/site-packages (from pandas==1.3) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/anaconda/lib/python3.7/site-packages (from pandas==1.3) (2018.9)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/ubuntu/.local/lib/python3.7/site-packages (from pandas==1.3) (1.21.6)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/anaconda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas==1.3) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "# Package import\n",
    "!pip install --user --upgrade pip\n",
    "!pip install --user pyspark\n",
    "!pip install --user pandas==1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Basic Functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "\n",
    "# PySpark\n",
    "import pyspark\n",
    "from pyspark.sql.functions import isnan, isnull\n",
    "from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
    "from pyspark.sql import Window, SparkSession, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Imputer,VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "source": [
    "## Python MLib tutorial----\n",
    "\n",
    "### MLib in Spark through DataFrame API - Simple Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer (\n",
    "inputCols= ['Age','Experience','salary'],\n",
    "outputCols = [\"{}_imputed\".format(c) for c in ['Age','Experience','salary']]).setStrategy('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureassembler = VectorAssembler(inputCols = ['Age','Experience'],outputCol = 'Independent_Feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = featureassembler.transform(df_pyspark)\n",
    "output.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pyspark = output.select(\"Independent_Feature\",\"salary\")\n",
    "final_pyspark.show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Split\n",
    "train_data,test_data=final_pyspark.randomSplit([0.75,0.25])\n",
    "regressor = LinearRegression(featuresCol = 'Independent_Feature',labelCol = \"salary\")\n",
    "regressor = regressor.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regressor.coefficients)\n",
    "print(regressor.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results=regressor.evaluate(test_data)\n",
    "pred_results.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results.meanAbsoluteError,pred_results.meanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pyspark and Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "cancer_df['target']=data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_df_pyspark = spark.createDataFrame(cancer_df) \n",
    "cancer_df_pyspark.printSchema()\n",
    "cancer_df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_df_pyspark.groupBy('target').count().show()\n",
    "# 1 means diagnosis of breast tissues benign (B)and 0 means, the diagnosis is malignant(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pyspark = output.select(\"Independent_Feature\",\"salary\")\n",
    "final_pyspark.show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "diabetes_df['target']=data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df_pyspark = spark.createDataFrame(diabetes_df) \n",
    "diabetes_df_pyspark.printSchema()\n",
    "diabetes_df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = list(diabetes_df_pyspark.columns)\n",
    "column_name.remove('target')\n",
    "featureassembler = VectorAssembler(inputCols = column_name,outputCol = 'Independent_Feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_diabetes_pyspark = featureassembler.transform(diabetes_df_pyspark).select(\"Independent_Feature\",\"target\")\n",
    "final_diabetes_pyspark.show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data=final_diabetes_pyspark.randomSplit([0.65,0.35])\n",
    "regressor = LinearRegression(featuresCol = 'Independent_Feature',labelCol = \"target\")\n",
    "regressor = regressor.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regressor.coefficients)\n",
    "print(regressor.intercept)\n",
    "pred_results=regressor.evaluate(test_data)\n",
    "pred_results.meanAbsoluteError,pred_results.meanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset was generated after the hackers hacked the servers to save the company data from such activities in future \n",
    "# forensic engineers. They grabbed some features that will give us relevant metadata about the type of hackers.\n",
    "\n",
    "# Here is a brief description of each feature:\n",
    "\n",
    "# Session connection time: This indicates the total time session existed in minutes.\n",
    "# Bytes transferred: will let us know how many megabytes were transferred during the session.\n",
    "# Kali trace used: This is a kind of flag variable that indicates whether the hacker used the Kali Linux operator.\n",
    "# Servers corrupted: How many servers got corrupted during the attack.\n",
    "# Pages corrupted: How many pages were accessed by them illegally.\n",
    "# Location: Though this meta information is also available with us, this one is of no use as hackers use VPNs\n",
    "# WPM typing speed: The typing speed of those criminals based on the available logs.\n",
    "# First, let’s understand what the company already knows. So, they are aware of the fact that 3 types of hackers might penetrate the attack. They are quite sure about the 2 of them, but they want us to know whether the third type of attacker was involved in this act of criminal or not.\n",
    "# One key thing we should know before moving forward, i.e., forensic engineers know that hackers trade-offs, which means the number of attacks was the same for each hacker. So if there are 3 types of hackers, three of them might have equally distributed the attacks. Otherwise, the third suspect would not have been involved this time.\n",
    "\n",
    "# Example: If all three types of attackers were the suspects, then for 100 attacks, each one will be responsible for 33."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/farhanafayez/PySpark-K-means-Clustering-ML.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark =SparkSession.builder.appName('Practise').getOrCreate()\n",
    "dataset = spark.read.csv(\"/mnt/PySpark/PySpark-K-means-Clustering-ML/hack_data.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vishualization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_group = dataset.groupBy(\"Location\").sum(\"Bytes Transferred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = loc_group.toPandas()[\"Location\"].values.tolist(),loc_group.toPandas()[\"sum(Bytes Transferred)\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40, 30), dpi=300)\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40, 30), dpi=300)\n",
    "plt.bar(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_group.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length((loc_group.select(\"Location\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = dataset.columns\n",
    "feature_cols.remove('Location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_assembler = VectorAssembler(inputCols =feature_cols,outputCol='features')\n",
    "final_data=vector_assembler.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol='features',outputCol='scaledFeatures',withStd=True,withMean=False)\n",
    "scalerModel =scaler.fit(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_final_data=scalerModel.transform(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_final_data..select(\"scaledFeatures\",\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions={}\n",
    "for k in range(2,9):\n",
    "    kmeans = KMeans(featuresCol='scaledFeatures',k=k)\n",
    "    model = kmeans.fit(cluster_final_data)\n",
    "    model = model.transform(cluster_final_data)\n",
    "    predictions[k]=model.groupBy('prediction').count()\n",
    "    k_evaluator = evaluator.evaluate(model)\n",
    "    print(\"With K={}\".format(k))\n",
    "    print(\"Error results  = \" + str(k_evaluator))\n",
    "    print('-'*53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in predictions.items():\n",
    "    print(\"With K={}\".format(key))\n",
    "    print((value.show()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inference: Here, we can see that an equal number of predictions exist in the case where several clusters are only 2; \n",
    "# hence we can conclude that the third type of attacker was not involved in hacking the company’s servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wssse_k3 = model_k3.computeCost(cluster_final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".select(\"Independent_Feature\",\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = ClusteringEvaluator()\n",
    "silhouette = evaluator.evaluate(model_k3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do one clustering\n",
    "# do one logistic reression\n",
    "# do one linear reression\n",
    "# do one classification\n",
    "DecisionTreeClassifier/Regressor (SPARK-19591), RandomForestClassifier/Regressor (SPARK-9478), GBTClassifier/Regressor (SPARK-9612), MulticlassClassificationEvaluator (SPARK-24101), RegressionEvaluator (SPARK-24102), BinaryClassificationEvaluator (SPARK-24103), BisectingKMeans (SPARK-30351), KMeans (SPARK-29967) and GaussianMixture (SPARK-30102).\n",
    "https://spark.apache.org/docs/latest/ml-guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}