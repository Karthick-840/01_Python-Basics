{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change preivous Pyspark linl\n",
    "# https://towardsdatascience.com/distributed-deep-learning-pipelines-with-pyspark-and-keras-a3a1c22b9239"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, ArrayType, BinaryType\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary import \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.image import ImageSchema\n",
    "from pyspark.sql.functions import lit\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spark session\n",
    "spark = SparkSession.builder.appName(‘DigitRecog’).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-2-ad80bff685f8>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-ad80bff685f8>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    spark = SparkSession.builder.appName(‘DigitRecog’).getOrCreate()\u001b[0m\n\u001b[0m                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# loaded image\n",
    "zero = ImageSchema.readImages(\"0\").withColumn(\"label\", lit(0))\n",
    "one = ImageSchema.readImages(\"1\").withColumn(\"label\", lit(1))\n",
    "two = ImageSchema.readImages(\"2\").withColumn(\"label\", lit(2))\n",
    "three = ImageSchema.readImages(\"3\").withColumn(\"label\", lit(3))\n",
    "four = ImageSchema.readImages(\"4\").withColumn(\"label\", lit(4))\n",
    "five = ImageSchema.readImages(\"5\").withColumn(\"label\", lit(5))\n",
    "six = ImageSchema.readImages(\"6\").withColumn(\"label\", lit(6))\n",
    "seven = ImageSchema.readImages(\"7\").withColumn(\"label\", lit(7))\n",
    "eight = ImageSchema.readImages(\"8\").withColumn(\"label\", lit(8))\n",
    "nine = ImageSchema.readImages(\"9\").withColumn(\"label\", lit(9))\n",
    "dataframes = [zero, one, two, three,four,\n",
    "             five, six, seven, eight, nine]\n",
    "# merge data frame\n",
    "df = reduce(lambda first, second: first.union(second), dataframes)\n",
    "# repartition dataframe \n",
    "df = df.repartition(200)\n",
    "# split the data-frame\n",
    "train, test = df.randomSplit([0.8, 0.2], 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from sparkdl import DeepImageFeaturizer\n",
    "# model: InceptionV3\n",
    "# extracting feature from images\n",
    "featurizer = DeepImageFeaturizer(inputCol=\"image\",\n",
    "                                 outputCol=\"features\",\n",
    "                                 modelName=\"InceptionV3\")\n",
    "# used as a multi class classifier\n",
    "lr = LogisticRegression(maxIter=5, regParam=0.03, \n",
    "                        elasticNetParam=0.5, labelCol=\"label\")\n",
    "# define a pipeline model\n",
    "sparkdn = Pipeline(stages=[featurizer, lr])\n",
    "spark_model = sparkdn.fit(train) # start fitting or training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# evaluate the model with test set\n",
    "evaluator = MulticlassClassificationEvaluator() \n",
    "tx_test = spark_model.transform(test)\n",
    "print('F1-Score ', evaluator.evaluate(tx_test, \n",
    "                                      {evaluator.metricName: 'f1'}))\n",
    "print('Precision ', evaluator.evaluate(tx_test,\n",
    "                                       {evaluator.metricName:                    'weightedPrecision'}))\n",
    "print('Recall ', evaluator.evaluate(tx_test, \n",
    "                                    {evaluator.metricName: 'weightedRecall'}))\n",
    "print('Accuracy ', evaluator.evaluate(tx_test, \n",
    "                                      {evaluator.metricName: 'accuracy'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get the result. It’s promising until now.\n",
    "\n",
    "F1-Score  0.8111782234361806\n",
    "Precision  0.8422058244785519\n",
    "Recall  0.8090909090909091\n",
    "Accuracy  0.8090909090909091\n",
    "Confusion Metrix\n",
    "Here, we’ll summarize the performance of a classification model using the confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.GnBu):\n",
    "plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = tx_test.select(\"label\")\n",
    "y_true = y_true.toPandas()\n",
    "y_pred = tx_test.select(\"prediction\")\n",
    "y_pred = y_pred.toPandas()\n",
    "cnf_matrix = confusion_matrix(y_true, y_pred,labels=range(10))\n",
    "Let’s visualize the Confusion Matrix\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.grid(False)\n",
    "# call pre defined function\n",
    "plot_confusion_matrix(cnf_matrix, classes=range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fig 3: Confusion Matrix for 10 Bengali Digits (0 to 9)\n",
    "Classification Report\n",
    "Here we can also get a report on the classification of each class by the evaluation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = [\"Class {}\".format(i) for i in range(10)]\n",
    "print(classification_report(y_true, y_pred, target_names = target_names))\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "def multiclass_roc_auc_score(y_test, y_pred, average=\"macro\"):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(y_test)\n",
    "    y_test = lb.transform(y_test)\n",
    "    y_pred = lb.transform(y_pred)\n",
    "    return roc_auc_score(y_test, y_pred, average=average)\n",
    "print('ROC AUC score:', multiclass_roc_auc_score(y_true,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.GnBu):\n",
    "plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, glob, os\n",
    "sys.path.extend(glob.glob(os.path.join(os.path.expanduser(\"~\"), \".ivy2/jars/*.jar\")))\n",
    "\n",
    "import findspark\n",
    "findspark.init('/home/innat/spark-2.4.0-bin-hadoop2.7')\n",
    "import pyspark # pyspark library\n",
    "import sparkdl # deep learning library : tensorflow backend \n",
    "from keras.applications import InceptionV3 # transfer learning using pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN model with spark and Elephas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
